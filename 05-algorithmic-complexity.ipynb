{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Complexity\n",
    "\n",
    "When designing programs, the key factor is correctness. We need accurate financial calculations, stable operating systems, and reliable embedded systems (e.g., in cars or airplanes). Performance is also crucial—warning systems must work in real time, applications should launch quickly, and databases should process transactions efficiently.\n",
    "\n",
    "Writing efficient programs is challenging since the simplest solution is not always the fastest. Sometimes, we increase conceptual complexity to reduce computational complexity. To do this effectively, we must understand how to estimate computational complexity.\n",
    "\n",
    "### Measuring Complexity\n",
    "\n",
    "The actual runtime of a program depends on:\n",
    "\n",
    "- The computer's speed,\n",
    "\n",
    "- The efficiency of the programming language implementation,\n",
    "\n",
    "- The specific input values.\n",
    "\n",
    "To avoid these dependencies, we measure complexity in terms of the number of basic operations performed instead of actual time. We use the **RAM (Random Access Machine) model**, where operations execute sequentially, each taking a fixed amount of time.\n",
    "\n",
    "### Running Time\n",
    "\n",
    "There are three main cases:\n",
    "\n",
    "- **Best case** – the shortest runtime for a given input size.\n",
    "\n",
    "- **Worst case** – the longest runtime for a given input size.\n",
    "\n",
    "- **Average case** – the expected runtime over all possible inputs.\n",
    "\n",
    "Worst-case analysis is most common, as it ensures the program meets time constraints in critical applications.\n",
    "\n",
    "### Asymptotic Notation\n",
    "\n",
    "To describe how runtime grows with input size, we use **asymptotic notation**, particularly **Big O notation**.\n",
    "\n",
    "Simplification rules:\n",
    "\n",
    "- Keep the term with the highest growth rate and discard others.\n",
    "\n",
    "- Ignore constant multipliers.\n",
    "\n",
    "**Example**: If $f(n)\\in \\mathcal{O}(n^2)$, it means the function grows no faster than $n^2$ for large $n$. In computer science, we often say \"$f(n)$ is $\\mathcal{O}(n^2)$\" as shorthand for stating its worst-case upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Analysis\n",
    "\n",
    "When analyzing the computational complexity of an algorithm, we focus on the number of operations performed. These include all value evaluations, such as addition, multiplication, logical checks, variable assignments, and so on. A preliminary look at computational complexity was in Notebook `02-iterative-programs.ipynb`, where we analyzed the number of operations in an algorithm that checks whether $n$ is a prime number. To optimize the number of computations, we skipped all cases above $\\frac{n}{2}$ and even numbers.\n",
    "\n",
    "For sums, the complexity is dominated by the largest term, so $\\mathcal{O}(n) + \\mathcal{O}(n) = \\mathcal{O}(n)$ and $\\mathcal{O}(n^2) + \\mathcal{O}(n) = \\mathcal{O}(n^2)$. When multiplying, the exponents sum up, meaning $\\mathcal{O}(n^2) \\times \\mathcal{O}(n^2)$ = $\\mathcal{O}(n^4)$.\n",
    "\n",
    "### Case Examples:\n",
    "\n",
    "- **$\\mathcal{O}(n + m)$**: This notation typically arises when an algorithm performs operations on two independent variables, `n` and `m`. For example, in a loop over two arrays, where one array has `n` elements and the other has `m` elements, the complexity would be the sum of both, $\\mathcal{O}(n + m)$. An example can be found in algorithms for graph traversal where the number of vertices and edges differ.\n",
    "\n",
    "- **$\\mathcal{O}(n \\times m)$**: This occurs when two loops iterate over two different ranges, one with `n` iterations and the other with `m` iterations. A common case is matrix multiplication, where two matrices of dimensions `n x m` are multiplied. This leads to a total of $\\mathcal{O}(n \\times m) = \\mathcal{O}(n \\times n) = \\mathcal{O}(n^2)$ operations.\n",
    "\n",
    "- **$\\mathcal{O}(4n^3 + 2)$**: In this case, we have a cubic term and a constant term. The complexity is dominated by the cubic term, so the expression simplifies to $\\mathcal{O}(n^3)$. Such cases may occur in algorithms involving multiple nested loops, where each loop operates over a range dependent on `n`. \n",
    "\n",
    "In recursive analysis, we expand the sum of operations for each recursion level until we reach the base case, leading to a formula of the form $T(n) = f(k) + T(g(n))$, where $g(n)$ can take various forms, such as $T(n/c)$, $T(n^d)$, $T(n-1)$, $T(n-\\text{const})$, $T(n/c^k)$, $T(c^n)$, or $T(b/n)$. Each of these cases is analyzed using either the iterative method (substituting successive values) or by applying **the recurrence theorem** (e.g., the Master Theorem). The final result depends on the dominant term, which is determined in Big-O notation.\n",
    "\n",
    "\n",
    "### Time Complexity\n",
    "\n",
    "The time complexity of the algorithm is read as the running time for an input of size $n$.\n",
    "\n",
    "**Example**\n",
    "\n",
    "As an example, consider a program that calculates $a^b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(a, b):\n",
    "    ans = 1                               # 1\n",
    "    while b > 0:                          # 5n\n",
    "        ans = ans * a\n",
    "        b = b - 1\n",
    "    return ans                            # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$T(b) = 1 + 5b + 1 = 5b + 2$\n",
    "\n",
    "$\\mathcal{O}(5n + 2) = \\mathcal{O}(n)$\n",
    "\n",
    "In its standard form, the computational complexity is $\\mathcal{O}(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(a, b):\n",
    "    if b <= 1:                            # 1\n",
    "        return a                          # 1\n",
    "    else:\n",
    "        return a * power(a, b - 1)        # 3 + T(b - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$T(b) = 1 + 3 + T(b - 1) = 1 + 3 + (1 + 3 + T(b - 2)) = 4k + T(b - k)$\n",
    "\n",
    "$b - k = 1 \\implies k = b - 1$\n",
    "\n",
    "$1 + 4(b - 1) + T(b - (b - 1)) = 1 + 4(b - 1) + T(1) = 1 + 4(b - 1) + 2 = 1 + 4b - 4 + 2 = 4b - 1$\n",
    "\n",
    "$\\mathcal{O}(4n - 1) = \\mathcal{O}(n)$\n",
    "\n",
    "In the recursive form, the computational complexity is also $\\mathcal{O}(n)$.\n",
    "\n",
    "However, there is a method to significantly reduce the running time of the algorithm by using the following formula.\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{power}(a, b) = a^b = \\begin{cases}\n",
    "    a \\times \\text{power}(a, b - 1), & \\text{ if } b \\text{ is odd} \\\\\n",
    "    \\text{power}(a \\times a, \\frac{b}{2}), & \\text{ if } b \\text{ is even}\n",
    "\\end{cases}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(a, b):\n",
    "    if b <= 1:                             # 1\n",
    "        return a                           # 1\n",
    "    else:\n",
    "        if b % 2 == 0:                     # 2\n",
    "            return power(a * a, b // 2)    # 3 + T(b // 2)\n",
    "        else:\n",
    "            return a * power(a, b - 1)     # 3 + T(b - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$T(b) = 3 + 3 + T(\\frac{b}{2}) = 3 + 3 + (3 + 3 + T(\\frac{b}{2^2})) = 6k + T(\\frac{b}{2^k})$\n",
    "\n",
    "$\\frac{b}{2^k} = 1 \\implies k = \\log_{2}{(b)}$\n",
    "\n",
    "$6\\log_{2}{(b)} + T(1) = 6\\log_{2}{(b)} + 2$\n",
    "\n",
    "$\\mathcal{O}(6\\log_{2}{(n)} + 2) = \\mathcal{O}(\\log{(n)})$\n",
    "\n",
    "In the reduced form, the computational complexity is $\\mathcal{O}(\\log{(n)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Complexity\n",
    "\n",
    "The space complexity of the algorithm is read as the memory needed for an input of size $n$.\n",
    "\n",
    "**Example**\n",
    "\n",
    "As an example, consider a program that calculates $a^b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(a, b):\n",
    "    ans = 1\n",
    "    while b > 0:\n",
    "        ans = ans * a\n",
    "        b = b - 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Complexity of Power Algorithm](images/complexity-1.png)\n",
    "\n",
    "Space complexity of the standard form is $\\mathcal{O}(1)$ because algorith updates the values in one given place in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(a, b):\n",
    "    if b <= 1:\n",
    "        return a\n",
    "    else:\n",
    "        return a * power(a, b - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Complexity of Power Algorithm](images/complexity-2.png)\n",
    "\n",
    "Space complexity of the standard form is $\\mathcal{O}(n)$ because algorith create the recursion tree of size $n$ in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(a, b):\n",
    "    if b <= 1:\n",
    "        return a\n",
    "    else:\n",
    "        if b % 2 == 0:\n",
    "            return power(a * a, b // 2)\n",
    "        else:\n",
    "            return a * power(a, b - 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Complexity of Power Algorithm](images/complexity-3.png)\n",
    "\n",
    "Space complexity of the standard form is $\\mathcal{O}(\\log{(n)})$ because algorith create the recursion tree of size $\\log{(n)}$ in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Complexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Complexity\n",
    "\n",
    "Constant time complexity, represented as $\\mathcal{O}(1)$, indicates that the algorithm's execution time does not change with the size of the input. Regardless of how much data is provided, the number of operations remains the same. Common examples of algorithms with $\\mathcal{O}(1)$ complexity include accessing an element in an array or performing a basic arithmetic operation. This kind of complexity is typically seen in simple data structure operations, such as inserting or deleting from a hash table. $\\mathcal{O}(1)$ is considered optimal as it offers the fastest execution, though its practical use is limited to operations that are inherently independent of input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant Time: 1\n"
     ]
    }
   ],
   "source": [
    "def constant(n):\n",
    "    iterations = 0\n",
    "    iterations = iterations + 1\n",
    "    return iterations\n",
    "\n",
    "print('Constant Time:', constant(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic Complexity\n",
    "\n",
    "Logarithmic complexity, denoted $\\mathcal{O}(\\log{(n)})$, is seen when an algorithm reduces the problem size by half (or some constant fraction) with each step. A classic example is binary search, where the search space is halved with each comparison. This type of complexity often arises in algorithms that divide and conquer, like those that use a binary tree or a heap structure. Logarithmic complexity is highly efficient, especially for large datasets, because it grows much slower than linear complexity. As a result, logarithmic complexity is often encountered in algorithms related to searching, sorting, and various tree-based data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logarithmic Time: 4\n"
     ]
    }
   ],
   "source": [
    "def logarithmic(n):\n",
    "    iterations = 0\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        iterations = iterations + 1\n",
    "        i = 2 * i\n",
    "    return iterations\n",
    "\n",
    "print('Logarithmic Time:', logarithmic(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Complexity\n",
    "\n",
    "Linear time complexity, represented as $\\mathcal{O}(n)$, means that the execution time grows directly in proportion to the size of the input. For example, iterating through all elements of an array or list to find a specific value or compute a sum is a linear-time operation. Algorithms with $\\mathcal{O}(n)$ complexity typically process each element of the input individually, leading to a predictable performance increase as the input size grows. While linear time is more efficient than higher complexities, it can still be costly for very large datasets. This complexity is common in algorithms like basic searching or summing operations where each element must be visited at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Time: 9\n"
     ]
    }
   ],
   "source": [
    "def linear(n):\n",
    "    iterations = 0\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        iterations = iterations + 1\n",
    "        i = i + 1\n",
    "    return iterations\n",
    "\n",
    "print('Linear Time:', linear(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Linear Complexity\n",
    "\n",
    "Log-linear complexity, denoted as $\\mathcal{O}(n \\log{(n)})$, occurs when an algorithm performs a logarithmic operation for each element in the input. One of the most well-known algorithms with $\\mathcal{O}(n \\log{(n)})$ complexity is merge sort, where the input is repeatedly divided into smaller chunks, and the results are merged back together. This complexity is often found in efficient sorting algorithms, as well as some divide-and-conquer algorithms. While it’s more efficient than quadratic time complexities like $\\mathcal{O}(n^2)$, it still involves more overhead compared to linear or logarithmic algorithms. It is widely considered to be optimal for general-purpose sorting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Lineae Time: 36\n"
     ]
    }
   ],
   "source": [
    "def log_linear(n):\n",
    "    iterations = 0\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        j = 1\n",
    "        while j < n:\n",
    "            iterations = iterations + 1\n",
    "            j = 2 * j\n",
    "        i = i + 1\n",
    "    return iterations\n",
    "\n",
    "print('Log-Lineae Time:', log_linear(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Complexity\n",
    "\n",
    "Polynomial time complexity, expressed as $\\mathcal{O}(n^k)$ for some constant $k$, means that the algorithm’s execution time increases at a rate proportional to a polynomial function of the input size. An example of this is the bubble sort algorithm, which has a time complexity of $\\mathcal{O}(n^2)$. Polynomial complexities are common in brute-force algorithms or when problems require exhaustive search or combinations of data. Although they are more efficient than exponential time, they can still become impractical for large inputs, especially when the exponent $k$ is greater than 1. These algorithms are often used when no better alternatives exist or when the problem size is small enough to be manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Time: 81\n"
     ]
    }
   ],
   "source": [
    "def polynomial(n):\n",
    "    iterations = 0\n",
    "    i = 1\n",
    "    while i < n:\n",
    "        j = 1\n",
    "        while j < n:\n",
    "            iterations = iterations + 1\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    return iterations\n",
    "\n",
    "print('Polynomial Time:', polynomial(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential Complexity\n",
    "\n",
    "Exponential time complexity, represented as $\\mathcal{O}(2^n)$ $\\big($ or $\\mathcal{O}(k^n)$ $\\big)$ or similar forms, describes an algorithm whose execution time doubles with each additional input element. Algorithms with exponential complexity are typically inefficient for even moderately large input sizes. A well-known example is the brute-force solution to the traveling salesman problem, where every possible route is explored. Exponential time complexities are considered impractical for large datasets because their execution time increases very rapidly as the input size grows. These algorithms are often seen in problems involving combinatorial optimization, where finding an exact solution is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Time: 512\n"
     ]
    }
   ],
   "source": [
    "def exponential(n):\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return exponential(n - 1) + exponential(n - 1)\n",
    "\n",
    "print('Polynomial Time:', exponential(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\mathcal{O}(1) & \\mathcal{O}\\big(\\log{(n)}\\big) & \\mathcal{O}(n) & \\mathcal{O}\\big(n\\log{(n)}\\big) & \\mathcal{O}(n^k) & \\mathcal{O}(k^n) \\\\\n",
    "\\hline\n",
    "1 & 4 & 9 & 36 & 81 & 512 \\\\\n",
    "\\hline\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of Time and Space Complexity Optimization in Algorithms\n",
    "\n",
    "Optimizing time and space complexity is a crucial aspect of designing efficient algorithms, as it directly impacts the performance of a program. In simple terms, time complexity refers to how the execution time of an algorithm grows as the size of the input increases, while space complexity refers to how much memory the algorithm requires in relation to the input size. Both of these factors play a significant role in determining the scalability and usability of software, especially when dealing with large datasets or real-time systems.\n",
    "\n",
    "![Complexity](images/complexity.png)\n",
    "\n",
    "### Time Complexity Optimization\n",
    "Time complexity optimization focuses on reducing the number of operations an algorithm performs as the input size grows. By improving the time complexity, we can significantly reduce the execution time of an algorithm, making it faster and more responsive. This is especially important in applications that require real-time processing, such as video games, financial systems, or web applications, where delays or slow responses can be detrimental. Algorithms with lower time complexity, such as those with logarithmic or linear time, are often preferred in scenarios where large amounts of data need to be processed quickly.\n",
    "\n",
    "### Space Complexity Optimization\n",
    "Space complexity optimization is concerned with minimizing the amount of memory an algorithm uses as it runs. This is particularly important in environments with limited resources, such as embedded systems or mobile devices, where memory is constrained. Efficient algorithms that use minimal memory not only perform better but also prevent memory overflow or crashes. Reducing space complexity often involves utilizing in-place operations, using more efficient data structures, or optimizing memory allocation patterns to minimize the algorithm's footprint.\n",
    "\n",
    "### Why Optimization Matters\n",
    "The need for optimization becomes more pronounced when algorithms are scaled up to handle larger datasets or when deployed in production environments where efficiency directly affects user experience and system stability. Algorithms with high time or space complexity may work fine on small inputs but can become prohibitively slow or memory-intensive as the input size grows. In contrast, well-optimized algorithms ensure that resources are used efficiently, enabling systems to handle larger volumes of data, operate within resource constraints, and maintain responsiveness.\n",
    "\n",
    "### Balancing Time and Space Complexity\n",
    "In some cases, there is a trade-off between time and space complexity. For example, a more memory-intensive algorithm might be faster, while a memory-efficient algorithm could be slower. The challenge is to find the right balance between the two, depending on the specific constraints and goals of the application. For instance, if an application requires quick responses but has ample memory, optimizing for time complexity might take precedence. Conversely, in environments with limited memory, such as embedded systems, minimizing space complexity might be the higher priority.\n",
    "\n",
    "**In conclusion**, optimizing both time and space complexity is essential for building efficient algorithms that scale well with larger inputs and perform well in real-world applications. Striking the right balance between these two factors is key to ensuring that software systems are both fast and resource-efficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
